/**
 * @example streaming_chat.cpp
 * @brief Streaming Chat With LLM Example
 * @version 0.1
 * @date 2025-09-09
 *
 * @copyright Copyright (c) 2025 Edge AI, LLC. All rights reserved.
 */
#include <agents-cpp/config_loader.h>
#include <agents-cpp/context.h>
#include <agents-cpp/logger.h>
#include <agents-cpp/tools/tool_registry.h>

#include <iostream>

using namespace agents;

// Streaming chat with LLM
Task<int> runStreamingChat(int argc, char* argv[]) {
    // Initialize the logger
    Logger::init(Logger::Level::INFO);

    // Get API key from .env, environment, or command line
    String api_key;
    auto& config = ConfigLoader::getInstance();

    // Try to get API key from config or environment
    api_key = config.get("GEMINI_API_KEY", "");

    // If not found, check command line
    if (api_key.empty() && argc > 1) {
        api_key = argv[1];
    }

    // Still not found, show error and exit
    if (api_key.empty()) {
        Logger::error("API key not found. Please:");
        Logger::error("1. Create a .env file with GEMINI_API_KEY=your_key, or");
        Logger::error("2. Set the GEMINI_API_KEY environment variable, or");
        Logger::error("3. Provide an API key as a command line argument");
        co_return EXIT_FAILURE;
    }

    // Create the context
    auto context = std::make_shared<Context>();

    // Configure the LLM
    auto llm = createLLM("google", api_key, "gemini-2.0-flash");

    // Configure LLM options
    LLMOptions options;
    options.temperature = (0.7);
    llm->setOptions(options);

    context->setLLM(llm);

    // Get user input
    Logger::info("Enter a question or task for the model (or 'exit' to quit):");
    String user_input;
    while (true) {
        Logger::info("> ");
        std::getline(std::cin, user_input);

        if (user_input == "exit" || user_input == "quit" || user_input == "q") {
            break;
        }

        if (user_input.empty()) {
            continue;
        }

        try {
            // Chat with the model with coroutines
            auto generator = context->streamChat(user_input);

            // Display the result as it arrives
            while (auto item = co_await generator.next()) {
                String chunk = *item;
                std::cout << chunk << std::flush;
            }
            std::cout << std::endl;
        } catch (const std::exception& e) {
            Logger::error("Error: {}", e.what());
        }
    }

    co_return EXIT_SUCCESS;
}

// Entry point
int main(int argc, char* argv[]) {
    // Use blockingWait to execute the coroutine
    return blockingWait(runStreamingChat(argc, argv));
}
